# -*- coding: utf-8 -*-
"""AppleAnalyzer.ai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LvTvskipQxdvmbigkxrhF2nuPCBOHuyA
"""

!pip install -q torch torchvision ultralytics gradio pillow groq

import torch
from torchvision import transforms
from PIL import Image,ImageDraw, ImageFont
from ultralytics import YOLO
import gradio as gr
from groq import Groq
import os
import torch.nn as nn
import torchvision.models as models
import cv2
import numpy as np

# STEP 3: Set Groq API Key
os.environ["GROQ_API_KEY"] = "gsk_NDNKTW3HdVUibk7TApAdWGdyb3FYOrjte4GIEvKJEoBkCCd"
groq_client = Groq(api_key=os.environ["GROQ_API_KEY"])

# STEP 4: Load Your Models
yolo_model = YOLO("/content/drive/MyDrive/detection_type/apple_detection/rotten_model/best.pt")

# Define the model architecture with correct final layer
resnet_model = models.resnet18(pretrained=False)
resnet_model.fc = nn.Linear(resnet_model.fc.in_features, 2)
model_path = '/content/drive/MyDrive/improved_apple_classifier_resnet18.pth'
resnet_model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))

# Set model to evaluation mode
resnet_model.eval()

# STEP 5: Define Preprocessing for ResNet18
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor()
])

labels = ["Fresh", "Rotten"]

# STEP 6: Define the Gradio Agent Function
def agent_pipeline_with_bbox(image):
    # Convert PIL image to OpenCV format for drawing
    frame_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)

    # YOLO Detection
    results = yolo_model(image)[0]

    detected_info = []

    if len(results.boxes) == 0:
        print("No apples detected")
        return image, [], "No apples detected in the image."

    for det in results.boxes:
        cls_id = int(det.cls[0])
        class_name = yolo_model.model.names[cls_id]

        if class_name.lower() != "apple":
          continue  # Skip anything that's not labeled as 'apple'

        x1, y1, x2, y2 = map(int, det.xyxy[0])
        conf = float(det.conf[0])

        # Crop the detected apple
        crop = frame_cv[y1:y2, x1:x2]
        pil_crop = Image.fromarray(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))

        # Classification by ResNet18
        input_tensor = transform(pil_crop).unsqueeze(0)
        with torch.no_grad():
            outputs = resnet_model(input_tensor)
            probs = torch.softmax(outputs, dim=1)
            confidence, predicted = torch.max(probs, 1)

        confidence = confidence.item()
        label = "Fresh" if predicted.item() == 0 else "Rotten"

        # Save detection info for later prompt
        detected_info.append({
            "label": label,
            "confidence": confidence
        })

        # Draw bounding box
        color = (0, 255, 0) if label == "Fresh" else (0, 0, 255)
        cv2.rectangle(frame_cv, (x1, y1), (x2, y2), color, 2)

        # Prepare text
        text = f"{label} ({confidence:.2f})"
        text_y = y1 - 10 if y1 - 10 > 10 else y1 + 20

        # Put text on image
        cv2.putText(frame_cv, text, (x1, text_y),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

    # Convert back to PIL
    annotated_image = Image.fromarray(cv2.cvtColor(frame_cv, cv2.COLOR_BGR2RGB))

    # Prepare prompt for LLaMA summarizing all detections
    detection_summary = ", ".join(
        [f"{info['label']} (conf: {info['confidence']:.2f})" for info in detected_info]
    )

    user_prompt = f"""
    The objects detected are apples.

    Detection results: {detection_summary}.

    Please provide explanations and suggestions based on these conditions in less that 100 words.
    """

    llm_response = groq_client.chat.completions.create(
        model="llama3-70b-8192",
        messages=[
            {"role": "system", "content": "You are an agricultural expert AI helping users understand fruit freshness."},
            {"role": "user", "content": user_prompt}
        ]
    )

    response_text = llm_response.choices[0].message.content.strip()

    return annotated_image, detected_info, response_text



# STEP 7: Create Gradio Interface
gr.Interface(
    fn=agent_pipeline_with_bbox,
    inputs=gr.Image(type="pil"),
    outputs=[
        gr.Image(label="Detected/Analyzed Apple"),
        gr.Textbox(label="Freshness"),
        gr.Textbox(label="LLaMA 3 Agent Advice")
    ],
    title="Apple Freshness AI Agent",
    description="Upload an apple image. YOLO detects it, ResNet classifies it, and LLaMA 3 (Groq) explains it."
).launch()

#  Install LangGraph + Required Libs (first-time setup)
!pip install -q langgraph langchain torch torchvision ultralytics gradio

# IMPORTS
import os
import torch
from torchvision import transforms, models
from PIL import Image
from ultralytics import YOLO
import cv2
import numpy as np
from groq import Groq
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode

# Set Groq API Key
os.environ["GROQ_API_KEY"] = "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"
groq_client = Groq(api_key=os.environ["GROQ_API_KEY"])

#Load YOLOv8 Model
yolo_model = YOLO("/content/drive/MyDrive/detection_type/apple_detection/rotten_model/best.pt")

# oad ResNet18 Classifier
resnet_model = models.resnet18(pretrained=False)
resnet_model.fc = torch.nn.Linear(resnet_model.fc.in_features, 2)
resnet_model.load_state_dict(torch.load('/content/drive/MyDrive/improved_apple_classifier_resnet18.pth', map_location='cpu'))
resnet_model.eval()

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor()
])
labels = ["Fresh", "Rotten"]

#Tools
def detect_apples(state):
    image = state['image']
    frame_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
    results = yolo_model(image)[0]
    detections = []

    for det in results.boxes:
        cls_id = int(det.cls[0])
        class_name = yolo_model.model.names[cls_id]
        if class_name.lower() != 'apple':
            continue
        x1, y1, x2, y2 = map(int, det.xyxy[0])
        crop = frame_cv[y1:y2, x1:x2]
        detections.append({
            "bbox": [x1, y1, x2, y2],
            "crop": Image.fromarray(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))
        })

    state['detections'] = detections
    return state

def classify_apples(state):
    CONF_THRESHOLD = 0.6

    for det in state['detections']:
        pil_crop = det['crop']
        img_tensor = transform(pil_crop).unsqueeze(0)

        with torch.no_grad():
            output = resnet_model(img_tensor)
            probs = torch.softmax(output, dim=1)
            conf, pred = torch.max(probs, 1)

        label = labels[pred.item()]
        confidence = conf.item()

        # Store results back into detection
        det['label'] = label
        det['confidence'] = round(confidence, 2)

        if confidence < CONF_THRESHOLD:
            print(f" Uncertain prediction ({label}, conf: {confidence:.2f})")

            # Extract bounding box coordinates
            x1, y1 = det.get('x1', 0), det.get('y1', 0)  # fallback to 0 if not present

            # Create directory if not exists
            os.makedirs(f"retrain/{label}", exist_ok=True)

            # Save the uncertain sample
            file_name = f"retrain/{label}/{label.lower()}_{int(confidence*100)}_{x1}_{y1}.jpg"
            pil_crop.save(file_name)
        else:
            print(f"{label} ({confidence:.2f})")

    return state

agent_memory = {
    "total_images_processed": 0,
    "last_summary": "None",
    "history": []
}
def summarize_with_llm(state):
    detection_summary = ", ".join([
        f"{det['label']} (conf: {det['confidence']})"
        for det in state['detections']
    ])

    # Add memory to reasoning
    user_prompt = f"""
You are an AI agricultural agent helping with apple quality assessment.

Context:
- This is the {agent_memory['total_images_processed'] + 1}th image analyzed.
- Previous advice: {agent_memory['last_summary']}
- Current detection summary: {detection_summary}

Goal:
Provide a short and actionable summary of the apple quality (less than 50 words).
Also suggest whether the apples should be consumed, stored, or discarded.
    """

    response = groq_client.chat.completions.create(
        model="llama3-70b-8192",
        messages=[
            {"role": "system", "content": "You are a helpful agricultural agent AI."},
            {"role": "user", "content": user_prompt}
        ]
    )

    summary = response.choices[0].message.content.strip()

    # Update memory
    agent_memory["total_images_processed"] += 1
    agent_memory["last_summary"] = summary
    agent_memory["history"].append({
        "image_num": agent_memory["total_images_processed"],
        "detections": detection_summary,
        "summary": summary
    })

    state['summary'] = summary
    return state

# Define Graph
from typing import TypedDict, List
from PIL import Image as PILImage

class AgentState(TypedDict):
    image: PILImage.Image
    detections: List[dict]
    summary: str

builder = StateGraph(AgentState)

builder.add_node("detection", detect_apples)
builder.add_node("classification", classify_apples)
builder.add_node("llm_summary", summarize_with_llm)

# Set edges
builder.set_entry_point("detection")
builder.add_edge("detection", "classification")
builder.add_edge("classification", "llm_summary")
builder.add_edge("llm_summary", END)

# Compile agent graph
agent_graph = builder.compile()

import gradio as gr

def run_agentic_pipeline(image):
    state = {"image": image}
    final_state = agent_graph.invoke(state)

    response_text = final_state['summary']
    detections = final_state['detections']

    # Annotate image
    annotated = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
    for det in detections:
        x1, y1, x2, y2 = det['bbox']
        label = det['label']
        conf = det['confidence']
        color = (0, 255, 0) if label == "Fresh" else (0, 0, 255)
        cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 2)
        cv2.putText(annotated, f"{label} ({conf})", (x1, y1 - 10 if y1 - 10 > 10 else y1 + 20),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
    final_image = Image.fromarray(cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB))

    # Build memory summary
    memory_info = f"Images analyzed: {agent_memory['total_images_processed']}\nLast advice: {agent_memory['last_summary']}"

    return final_image, response_text, memory_info


# Gradio UI
gr.Interface(
    fn=run_agentic_pipeline,
    inputs=gr.Image(type="pil"),
    outputs=[
        gr.Image(label="Analyzed Image"),

        gr.Textbox(label="LLaMA 3 Advice"),

        gr.Textbox(label="Agent Memory")
    ],
    title="Agentic Apple Freshness Analyzer",
    description="YOLO detects, ResNet classifies, and Groq LLaMA 3 explains apple freshness."
).launch()

